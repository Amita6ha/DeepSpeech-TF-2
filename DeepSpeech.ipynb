{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DeepSpeech.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "eU1MgZd_69_r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "path = \"/content/drive/My Drive\"\n",
        "os.chdir(path)\n",
        "\n",
        "import csv\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow.compat.v1 as tfv1\n",
        "import scipy.io.wavfile as wav\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import Model\n",
        "\n",
        "!pip install python_speech_features\n",
        "from python_speech_features import mfcc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZvXCetmjoiS9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# definition of DeepSpeech architecture\n",
        "class DeepSpeech(Model):\n",
        "  def __init__(self, n_hidden, n_cell_dim, n_hidden_out, n_context, n_input):\n",
        "    super(DeepSpeech, self).__init__()\n",
        "    self.dense1 = layers.Dense(n_hidden, activation='relu')\n",
        "    self.dense2 = layers.Dense(n_hidden, activation='relu')\n",
        "    self.dense3 = layers.Dense(n_cell_dim, activation='relu')\n",
        "    self.lstm = layers.LSTM(n_cell_dim, return_sequences=True)\n",
        "    self.dense5 = layers.Dense(n_hidden, activation='relu')\n",
        "    self.dense_out = layers.Dense(n_hidden_out)\n",
        "\n",
        "    self.n_hidden = n_hidden\n",
        "    self.n_cell_dim = n_cell_dim\n",
        "    self.n_hidden_out = n_hidden_out\n",
        "    self.n_context = n_context\n",
        "    self.n_input = n_input\n",
        "\n",
        "  def create_overlapping_windows(self, batch_x, n_context, n_input):\n",
        "    batch_size = tf.shape(input=batch_x)[0]\n",
        "    window_width = 2 * n_context + 1\n",
        "    num_channels = n_input\n",
        "\n",
        "    # Create a constant convolution filter using an identity matrix, so that the\n",
        "    # convolution returns patches of the input tensor as is, and we can create\n",
        "    # overlapping windows over the MFCCs.\n",
        "    eye_filter = tf.constant(np.eye(window_width * num_channels).\n",
        "                             reshape(window_width, num_channels, window_width * num_channels), tf.float32)\n",
        "\n",
        "    # Create overlapping windows\n",
        "    batch_x = tf.nn.conv1d(input=batch_x, filters=eye_filter, stride=1, padding='SAME')\n",
        "\n",
        "    # Remove dummy depth dimension and reshape into [batch_size, n_windows, window_width, n_input]\n",
        "    batch_x = tf.reshape(batch_x, [batch_size, -1, window_width, num_channels])\n",
        "\n",
        "    return batch_x\n",
        "\n",
        "  def call(self, x):\n",
        "    batch_size = tf.shape(input=x)[0]\n",
        "\n",
        "    x = self.create_overlapping_windows(x, self.n_context, self.n_input)\n",
        "    x = tf.reshape(x, [-1, self.n_input + 2 * self.n_input * self.n_context])\n",
        "\n",
        "    x = self.dense1(x)\n",
        "    x = self.dense2(x)\n",
        "    x = self.dense3(x)\n",
        "\n",
        "    x = tf.reshape(x, [batch_size, -1, self.n_hidden])\n",
        "\n",
        "    x = self.lstm(x)\n",
        "\n",
        "    x = tf.reshape(x, [-1, self.n_cell_dim])\n",
        "\n",
        "    x = self.dense5(x)\n",
        "    x = self.dense_out(x)\n",
        "\n",
        "    x = tf.reshape(x, [batch_size, -1, self.n_hidden_out])\n",
        "    x = tf.transpose(x, [1, 0, 2]) # transpose to time major\n",
        "    \n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLQRgxf584ke",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# helpers\n",
        "def sparse_tuple_from(sequences, dtype=np.int32):\n",
        "  \"\"\"Create a sparse representention of x.\n",
        "  Args:\n",
        "    sequences: a list of lists of type dtype where each element is a sequence\n",
        "  Returns:\n",
        "    A tuple with (indices, values, shape)\n",
        "  \"\"\"\n",
        "\n",
        "  indices = []\n",
        "  values = []\n",
        "\n",
        "  for n, seq in enumerate(sequences):\n",
        "    indices.extend(zip([n]*len(seq), range(len(seq))))\n",
        "    values.extend(seq)\n",
        "\n",
        "    indices = np.asarray(indices, dtype=np.int64)\n",
        "    values = np.asarray(values, dtype=dtype)\n",
        "    shape = np.asarray([len(sequences), np.asarray(indices).max(0)[1]+1], dtype=np.int64)\n",
        "\n",
        "  return indices, values, shape\n",
        "\n",
        "# The following code is from: http://hetland.org/coding/python/levenshtein.py\n",
        "\n",
        "# This is a straightforward implementation of a well-known algorithm, and thus\n",
        "# probably shouldn't be covered by copyright to begin with. But in case it is,\n",
        "# the author (Magnus Lie Hetland) has, to the extent possible under law,\n",
        "# dedicated all copyright and related and neighboring rights to this software\n",
        "# to the public domain worldwide, by distributing it under the CC0 license,\n",
        "# version 1.0. This software is distributed without any warranty. For more\n",
        "# information, see <http://creativecommons.org/publicdomain/zero/1.0>\n",
        "\n",
        "def levenshtein(a, b):\n",
        "  \"Calculates the Levenshtein distance between a and b.\"\n",
        "  n, m = len(a), len(b)\n",
        "  if n > m:\n",
        "    # Make sure n <= m, to use O(min(n,m)) space\n",
        "    a, b = b, a\n",
        "    n, m = m, n\n",
        "\n",
        "  current = list(range(n+1))\n",
        "  for i in range(1, m+1):\n",
        "    previous, current = current, [i]+[0]*n\n",
        "    for j in range(1, n+1):\n",
        "      add, delete = previous[j]+1, current[j-1]+1\n",
        "      change = previous[j-1]\n",
        "      if a[j-1] != b[i-1]:\n",
        "        change = change + 1\n",
        "      current[j] = min(add, delete, change)\n",
        "\n",
        "  return current[n]\n",
        "\n",
        "def WER(truth, hypothesis):\n",
        "  return levenshtein(truth.split(), hypothesis.split()) / len(truth.split())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmS5tan7BF8M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# prepare dataset\n",
        "\n",
        "# Constants\n",
        "SPACE_TOKEN = '<space>'\n",
        "SPACE_INDEX = 0\n",
        "FIRST_INDEX = ord('a') - 1\n",
        "\n",
        "# Some configs\n",
        "num_features = 13\n",
        "num_classes = ord('z') - ord('a') + 1 + 1 + 1\n",
        "\n",
        "# import data\n",
        "!python DeepSpeech/import_ldc93s1.py DeepSpeech/data/ldc93s1\n",
        "\n",
        "train_files = \"DeepSpeech/data/ldc93s1/ldc93s1.csv\"\n",
        "\n",
        "# TO BE CONTINUED\n",
        "with open(train_files) as f:\n",
        "  reader = csv.reader(f)\n",
        "  first_row = next(reader)\n",
        "  for row in reader:\n",
        "    audio_file = row[0]\n",
        "    src_transcript = row[2]\n",
        "\n",
        "fs, audio = wav.read(audio_file)\n",
        "feature = mfcc(audio, samplerate=fs)\n",
        "feature = np.asarray(feature[np.newaxis, :])\n",
        "feature = (feature - np.mean(features))/np.std(feature)\n",
        "seq_len = [feature.shape[1]]\n",
        "\n",
        "#transcript = ' '.join(transcript.strip().lower().split(' ')[2:]).replace('.', '')\n",
        "transcript = src_transcript.replace(' ', '  ')\n",
        "transcript = transcript.split(' ')\n",
        "transcript = np.hstack([SPACE_TOKEN if x == '' else list(x) for x in transcript])\n",
        "transcript = np.asarray([SPACE_INDEX if x == SPACE_TOKEN else ord(x) - FIRST_INDEX for x in transcript])\n",
        "transcript = sparse_tuple_from([transcript])\n",
        "transcript = tf.sparse.SparseTensor(transcript[0],transcript[1],transcript[2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AepW9ib20kit",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# training and testing\n",
        "model = DeepSpeech(n_hidden=100,n_cell_dim=100,n_hidden_out=num_classes,n_context=7,n_input=num_features)\n",
        "\n",
        "loss_fn = tfv1.nn.ctc_loss\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
        "\n",
        "@tf.function\n",
        "def train(feature, transcript, sequence_length):\n",
        "  with tf.GradientTape() as tape:\n",
        "    # training=True is only needed if there are layers with different\n",
        "    # behavior during training versus inference (e.g. Dropout).\n",
        "    prediction = model(feature, training=True)\n",
        "    loss = loss_fn(transcript, prediction, sequence_length)\n",
        "  gradients = tape.gradient(loss, model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "  train_loss(loss)\n",
        "\n",
        "def test(feature, transcript, sequence_length, src_transcript):\n",
        "  # training=False is only needed if there are layers with different\n",
        "  # behavior during training versus inference (e.g. Dropout).\n",
        "  prediction = model(feature, training=False)\n",
        "  loss = loss_fn(transcript, prediction, sequence_length)\n",
        "\n",
        "  test_loss(loss)\n",
        "\n",
        "  decoded,_ = tf.nn.ctc_beam_search_decoder(prediction, sequence_length, beam_width = 500)\n",
        "  decoded = decoded[0].values.numpy()\n",
        "  res_transcript = ''.join([chr(x) for x in np.asarray(decoded) + FIRST_INDEX])\n",
        "  res_transcript = res_transcript.replace(chr(ord('z') + 1), '')\n",
        "  res_transcript = res_transcript.replace(chr(ord('a') - 1), ' ')\n",
        "\n",
        "  wer = WER(src_transcript, res_transcript)\n",
        "  \n",
        "  return res_transcript, wer\n",
        "\n",
        "EPOCHS = 200\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  # Reset the metrics at the start of the next epoch\n",
        "  train_loss.reset_states()\n",
        "  test_loss.reset_states()\n",
        "\n",
        "  train(feature, transcript, seq_len)\n",
        "\n",
        "  res_transcript, wer = test(feature, transcript, seq_len, src_transcript)\n",
        "\n",
        "  template = 'Epoch {}, TRAIN Loss: {}, Test Loss: {}, WER: {} \\n src: {} \\n res: {}'\n",
        "  print(template.format(epoch + 1,\n",
        "            train_loss.result(),\n",
        "            test_loss.result(),\n",
        "            wer,\n",
        "            src_transcript,\n",
        "            res_transcript))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}